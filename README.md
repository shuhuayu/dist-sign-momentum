# dist-sign-momentum

This is an implementation of the distributed sign momentum algorithm proposed in our paper: [Distributed Sign Momentum with Local Steps for Training Transformers](https://arxiv.org/abs/2411.17866). Our code is mainly based on [fairscale](https://github.com/facebookresearch/fairscale) and [nanoGPT](https://github.com/karpathy/nanoGPT). We implemented our algorithm in the fairscale framework, and built example use cases using nanoGPT. Our experiments show that dist-sign-momentum consistently outperforms than the built-in [SlowMo](https://arxiv.org/abs/1910.00643) method in distributed training of GPT models with multiple local steps. This makes our algorithm particularly well-suited for scenarios where communication is prohibitive and reducing synchronization frequency is essential. 